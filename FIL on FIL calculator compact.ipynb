{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from prophet import Prophet\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "%config InlineBackend.figure_format = 'retina' \n",
    "\n",
    "import os\n",
    "import logging\n",
    "logging.getLogger('fbprophet').setLevel(logging.WARNING) # Silence Prophet if nothing major happens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def height_to_unixepoch(height: int) -> int:\n",
    "    return height * 30 + 1598306400\n",
    "\n",
    "def height_to_timestamp(height: int) -> datetime:\n",
    "    return datetime.fromtimestamp(height_to_unixepoch(height))\n",
    "\n",
    "def unixepoch_to_height(timestamp):\n",
    "    return (timestamp - 1598306400)/30\n",
    "\n",
    "def datetime_to_height(year:int, month:int, day:int) -> float:\n",
    "    dd = int(datetime(year, month, day).strftime('%s'))\n",
    "    return int((dd - 1598306400)/30)\n",
    "\n",
    "# Some overheads to suppress Prophet output\n",
    "# https://github.com/facebook/prophet/issues/223#issuecomment-326455744\n",
    "class suppress_stdout_stderr(object):\n",
    "    '''\n",
    "    A context manager for doing a \"deep suppression\" of stdout and stderr in\n",
    "    Python, i.e. will suppress all print, even if the print originates in a\n",
    "    compiled C/Fortran sub-function.\n",
    "       This will not suppress raised exceptions, since exceptions are printed\n",
    "    to stderr just before a script exits, and after the context manager has\n",
    "    exited (at least, I think that is why it lets exceptions through).\n",
    "\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        # Open a pair of null files\n",
    "        self.null_fds = [os.open(os.devnull, os.O_RDWR) for x in range(2)]\n",
    "        # Save the actual stdout (1) and stderr (2) file descriptors.\n",
    "        self.save_fds = [os.dup(1), os.dup(2)]\n",
    "\n",
    "    def __enter__(self):\n",
    "        # Assign the null pointers to stdout and stderr.\n",
    "        os.dup2(self.null_fds[0], 1)\n",
    "        os.dup2(self.null_fds[1], 2)\n",
    "\n",
    "    def __exit__(self, *_):\n",
    "        # Re-assign the real stdout/stderr back to (1) and (2)\n",
    "        os.dup2(self.save_fds[0], 1)\n",
    "        os.dup2(self.save_fds[1], 2)\n",
    "        # Close the null files\n",
    "        for fd in self.null_fds + self.save_fds:\n",
    "            os.close(fd)    \n",
    "\n",
    "def interpolate_predictions(df, fcst, feature, yhat='yhat'):\n",
    "    '''\n",
    "    Takes the daily predictions from Prophet and interpolates it at the epoch level.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df (pd.DataFrame): original dataframe, sampled at epoch frequency\n",
    "    fcst (pd.DataFrame): output dataframe from Prophet (must have columns ds, yhat, yhat_lower, yhat_upper)\n",
    "    yhat (string): whether to use Prophet's median ('yhat'), optimistic ('yhat_upper') or pessimistic ('yhat_lower') prediction\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df_new (pd.DataFrame): interpolates daily Prophet prediction into an epoch-wise prediction. When feature \n",
    "                           is 'raw_bytes_network_power',  this function will also calculate the network time \n",
    "                           in epoch units. \n",
    "    '''\n",
    "\n",
    "    df_p = fcst[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].iloc[-300:]\n",
    "\n",
    "    if feature == 'raw_bytes_network_power':\n",
    "\n",
    "        assert 'raw_bytes_network_power' in df.columns, \"raw_bytes_network_power not in epoch-level dataframe\"\n",
    "\n",
    "        # Populate a new epoch-wise dataframe for the forward predictions\n",
    "        df_new = pd.DataFrame()\n",
    "        df_new['height'] = np.arange(df.height.max(), 300*2880+df.height.max())\n",
    "        df_new['datetime'] = df_new['height'].apply(height_to_timestamp)\n",
    "\n",
    "        # Impute the daily prediction at 12 midnight of each day\n",
    "        midnight_condition = (df_new['datetime'].dt.hour == 0) & (df_new['datetime'].dt.minute == 0) & (df_new['datetime'].dt.second == 0)\n",
    "        df_new['raw_bytes_network_power'] = np.nan\n",
    "\n",
    "        midnight_idx = np.argwhere(midnight_condition.values>0).reshape(1,-1)[0]\n",
    "        for i,idx in enumerate(midnight_idx):\n",
    "            df_new.loc[idx, 'raw_bytes_network_power'] = df_p.iloc[-300:][yh].values[i]\n",
    "\n",
    "        # Patch the first row of the extrapolated dataframe to the last row of the actual chain data, to facilitate interpolation\n",
    "        # There will therefore be 301 interpolated points in the epoch-wise extrapolated dataframe\n",
    "        df_new.loc[0, 'raw_bytes_network_power'] = df['raw_bytes_network_power'].iloc[-1]\n",
    "        df_new['raw_bytes_network_power'] = df_new['raw_bytes_network_power'].interpolate(method='linear', limit_direction='forward')\n",
    "\n",
    "        dynamic_baseline = lambda epoch: df.new_baseline_power.iloc[0] * np.exp(np.log(2)/525600/2*epoch)\n",
    "        g = np.log(2)/31536000*30 # the growth rate in epoch units\n",
    "        b0 = 2.888888 * 2**60 / 2**50\n",
    "\n",
    "        # Don't forget to drop the duplicate row in df_new before concat\n",
    "        df_out = pd.concat([df[['height', 'datetime', 'raw_bytes_network_power']], \n",
    "                            df_new[['height', 'datetime', 'raw_bytes_network_power']].iloc[1:]]) \n",
    "        df_out = df_out.reset_index(drop=True)\n",
    "        df_out['baseline_power'] = dynamic_baseline(np.arange(len(df_out)))\n",
    "        df_out['cumsum_capped_rb_power'] = df_out[['raw_bytes_network_power', 'baseline_power']].min(axis=1).cumsum()\n",
    "        df_out['est_network_height'] = 15410 + 1.055*np.log(df_out['cumsum_capped_rb_power']*g/b0 + 1)/g   \n",
    "\n",
    "    else: \n",
    "\n",
    "        assert feature in df.columns, \"%s not in epoch-level dataframe\" % feature\n",
    "\n",
    "        # Populate a new epoch-wise dataframe for the forward predictions\n",
    "        df_new = pd.DataFrame()\n",
    "        df_new['height'] = np.arange(df.height.max(), 300*2880+df.height.max())\n",
    "        df_new['datetime'] = df_new['height'].apply(height_to_timestamp)\n",
    "\n",
    "        # Impute the daily prediction at 12 midnight of each day\n",
    "        midnight_condition = (df_new['datetime'].dt.hour == 0) & (df_new['datetime'].dt.minute == 0) & (df_new['datetime'].dt.second == 0)\n",
    "        df_new[feature] = np.nan\n",
    "\n",
    "        midnight_idx = np.argwhere(midnight_condition.values>0).reshape(1,-1)[0]\n",
    "        for i,idx in enumerate(midnight_idx):\n",
    "            df_new.loc[idx, feature] = df_p.iloc[-300:]['yhat'].values[i]\n",
    "\n",
    "        # Patch the first row of the extrapolated dataframe to the last row of the actual chain data, to facilitate interpolation\n",
    "        # There will therefore be 301 interpolated points in the epoch-wise extrapolated dataframe\n",
    "        df_new.loc[0, feature] = df[feature].iloc[-1]\n",
    "        df_new[feature] = df_new[feature].interpolate(method='linear', limit_direction='forward')\n",
    "\n",
    "        # Don't forget to drop the duplicate row in df_new before concat\n",
    "        df_out = pd.concat([df[['height', 'datetime', feature]], \n",
    "                            df_new[['height', 'datetime', feature]].iloc[1:]]) \n",
    "        df_out = df_out.reset_index(drop=True)\n",
    "\n",
    "        if feature == 'qa_bytes_network_power':\n",
    "            # if the feature is QA bytes network power, re-calculate the smoothed velocity and position estimates for the entire extrapolated df\n",
    "\n",
    "            position = df['qa_smoothed_position_estimate'].iloc[0]\n",
    "            velocity = df['qa_smoothed_velocity_estimate'].iloc[1]\n",
    "            dt = 1\n",
    "            alpha = 0.000164\n",
    "            beta = 0.000115\n",
    "            qa_recalc_pos = []\n",
    "            qa_recalc_vel = []\n",
    "            for k in range(len(df_out)):\n",
    "                position = position + velocity * dt\n",
    "                velocity = velocity\n",
    "                residual = df_out['qa_bytes_network_power'].iloc[k] - position\n",
    "                position = position + alpha * residual\n",
    "                velocity = velocity + (beta/dt) * residual\n",
    "                qa_recalc_pos.append(position)\n",
    "                qa_recalc_vel.append(velocity)\n",
    "            df_out['qa_smoothed_position_estimate'] = qa_recalc_pos\n",
    "            df_out['qa_smoothed_velocity_estimate'] = qa_recalc_vel\n",
    "\n",
    "\n",
    "    return df_out\n",
    "\n",
    "def calculate_block_reward(df):\n",
    "\n",
    "    '''\n",
    "    Given a fully extrapolated epoch-level dataframe, calculate block rewards received per day. \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df (pd.DataFrame): extrapolated epoch-level dataframe\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df (pd.DataFrame): dataframe with simple reward, baseline reward, M (minted supply target), w (reward per wincount)\n",
    "    '''\n",
    "\n",
    "    FIL_BASE = 2e9          # only this amount in FIL will ever be issued\n",
    "    M_infty = 0.55*FIL_BASE # asymptotic number of tokens\n",
    "    lda = np.log(2)/6       # the minting rate corresponding to a 6 year half life\n",
    "    gamma = 0.7             # the minting ratio assigned to hybrid minting\n",
    "    conv_factor = 525600*2  # number of epochs in a year\n",
    "\n",
    "    df['delta_t'] = ( df['height'] - df['height'].iloc[0])\n",
    "    df['delta_t_bl'] = ( df['est_network_height'] - df['est_network_height'].iloc[0])    \n",
    "    df['simple_reward'] = (1-gamma)*M_infty*gamma*(1-np.exp(-lda * df['delta_t']/conv_factor))\n",
    "    df['baseline_reward'] = gamma*M_infty*gamma*(1-np.exp(-lda * df['delta_t_bl']/conv_factor))\n",
    "    df['M'] =  df['simple_reward'] + df['baseline_reward']\n",
    "    df['M_diff'] = np.gradient(df['M'].rolling(2880).mean())\n",
    "    df['w'] = df['M_diff'] / 5 \n",
    "\n",
    "    return df    \n",
    "\n",
    "def ratio_extract(num_p, num_v, denom_p, denom_v, delta_T):\n",
    "    '''\n",
    "    Evaluate the ratio of two variables by the alpha-beta filter\n",
    "    '''\n",
    "    x2a = np.log(denom_p + denom_v) \n",
    "    x2b = np.log(denom_p + denom_v + denom_v * delta_T)\n",
    "    m1 = denom_v * num_p * (x2b - x2a)\n",
    "    m2 = num_v * (denom_p * (x2a - x2b) + denom_v * delta_T)\n",
    "    return (m1 + m2) / denom_v**2    \n",
    "\n",
    "def predict_on_dataframe(df, column_name = 'raw_bytes_network_power'):\n",
    "    dc = df[['datetime', column_name]].reset_index(drop=True)\n",
    "    dc.columns = ['ds', 'y']\n",
    "    \n",
    "    m = Prophet()\n",
    "    with suppress_stdout_stderr():\n",
    "        m.fit(dc)\n",
    "    future = m.make_future_dataframe(periods=300)\n",
    "    fcst = m.predict(future)\n",
    "    return fcst\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/df_height.csv', index_col=[0])\n",
    "df['datetime'] = df['height'].apply(height_to_timestamp)\n",
    "\n",
    "# 1. A new dataframe that's sampled exactly at midnight\n",
    "# it should occur at intervals of 2880 epochs or so \n",
    "dfc = df[(df['datetime'].dt.hour == 0) & (df['datetime'].dt.minute == 0) & (df['datetime'].dt.second == 0)]\n",
    "\n",
    "# 2. Make fresh (daily) predictions of QA/RB network power and circulating FIL\n",
    "fcst = predict_on_dataframe(dfc, 'raw_bytes_network_power')\n",
    "fcst_qa = predict_on_dataframe(dfc, 'qa_bytes_network_power')\n",
    "fcst_circ = predict_on_dataframe(dfc, 'circulating_fil')\n",
    "\n",
    "# Then \n",
    "df_new = interpolate_predictions(df, fcst, 'raw_bytes_network_power')\n",
    "df_new_qa = interpolate_predictions(df, fcst_qa, 'qa_bytes_network_power')\n",
    "df_new_circ = interpolate_predictions(df, fcst_circ, 'circulating_fil')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5752224e654bc799f449297a56840520f80f4208903c6117e39d7e647da611dd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
