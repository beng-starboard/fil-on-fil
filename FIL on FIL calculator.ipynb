{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9cf1b2fa-c90e-4839-9b9d-2025a63784f6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from prophet import Prophet\n",
    "from datetime import datetime\n",
    "from scipy.stats import poisson\n",
    "import matplotlib.pyplot as plt\n",
    "%config InlineBackend.figure_format = 'retina' \n",
    "\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import logging\n",
    "logging.getLogger('fbprophet').setLevel(logging.WARNING) # Silence Prophet if nothing major happens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "608b4cf3-7e53-41ce-9fa1-81cb6eff9281",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def height_to_unixepoch(height: int) -> int:\n",
    "    return height * 30 + 1598306400\n",
    "\n",
    "def height_to_timestamp(height: int) -> datetime:\n",
    "    return datetime.fromtimestamp(height_to_unixepoch(height))\n",
    "\n",
    "def unixepoch_to_height(timestamp):\n",
    "    return (timestamp - 1598306400)/30\n",
    "\n",
    "def datetime_to_height(year:int, month:int, day:int) -> float:\n",
    "    dd = int(datetime(year, month, day).strftime('%s'))\n",
    "    return int((dd - 1598306400)/30)\n",
    "\n",
    "# Some overheads to suppress Prophet output\n",
    "# https://github.com/facebook/prophet/issues/223#issuecomment-326455744\n",
    "class suppress_stdout_stderr(object):\n",
    "    '''\n",
    "    A context manager for doing a \"deep suppression\" of stdout and stderr in\n",
    "    Python, i.e. will suppress all print, even if the print originates in a\n",
    "    compiled C/Fortran sub-function.\n",
    "       This will not suppress raised exceptions, since exceptions are printed\n",
    "    to stderr just before a script exits, and after the context manager has\n",
    "    exited (at least, I think that is why it lets exceptions through).\n",
    "\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        # Open a pair of null files\n",
    "        self.null_fds = [os.open(os.devnull, os.O_RDWR) for x in range(2)]\n",
    "        # Save the actual stdout (1) and stderr (2) file descriptors.\n",
    "        self.save_fds = [os.dup(1), os.dup(2)]\n",
    "\n",
    "    def __enter__(self):\n",
    "        # Assign the null pointers to stdout and stderr.\n",
    "        os.dup2(self.null_fds[0], 1)\n",
    "        os.dup2(self.null_fds[1], 2)\n",
    "\n",
    "    def __exit__(self, *_):\n",
    "        # Re-assign the real stdout/stderr back to (1) and (2)\n",
    "        os.dup2(self.save_fds[0], 1)\n",
    "        os.dup2(self.save_fds[1], 2)\n",
    "        # Close the null files\n",
    "        for fd in self.null_fds + self.save_fds:\n",
    "            os.close(fd)    \n",
    "\n",
    "def interpolate_predictions(df, fcst, feature, yhat='yhat', n_days=450):\n",
    "    '''\n",
    "    Takes the daily predictions from Prophet and interpolates it at the epoch level.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df (pd.DataFrame): original dataframe, sampled at epoch frequency\n",
    "    fcst (pd.DataFrame): output dataframe from Prophet (must have columns ds, yhat, yhat_lower, yhat_upper)\n",
    "    yhat (string): whether to use Prophet's median ('yhat'), optimistic ('yhat_upper') or pessimistic ('yhat_lower') prediction\n",
    "    n_days (int): number of days to predict ahead\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df_new (pd.DataFrame): interpolates daily Prophet prediction into an epoch-wise prediction. When feature \n",
    "                           is 'raw_bytes_network_power',  this function will also calculate the network time \n",
    "                           in epoch units. \n",
    "    '''\n",
    "\n",
    "    df_p = fcst[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].iloc[-n_days:]\n",
    "\n",
    "    if feature == 'raw_bytes_network_power':\n",
    "\n",
    "        assert 'raw_bytes_network_power' in df.columns, \"raw_bytes_network_power not in epoch-level dataframe\"\n",
    "\n",
    "        # Populate a new epoch-wise dataframe for the forward predictions\n",
    "        df_new = pd.DataFrame()\n",
    "        df_new['height'] = np.arange(df.height.max(), n_days*2880+df.height.max())\n",
    "        df_new['datetime'] = df_new['height'].apply(height_to_timestamp)\n",
    "\n",
    "        # Impute the daily prediction at 12 midnight of each day\n",
    "        midnight_condition = (df_new['datetime'].dt.hour == 0) & (df_new['datetime'].dt.minute == 0) & (df_new['datetime'].dt.second == 0)\n",
    "        df_new['raw_bytes_network_power'] = np.nan\n",
    "\n",
    "        midnight_idx = np.argwhere(midnight_condition.values>0).reshape(1,-1)[0]\n",
    "        for i,idx in enumerate(midnight_idx):\n",
    "            df_new.loc[idx, 'raw_bytes_network_power'] = df_p.iloc[-n_days:][yhat].values[i]\n",
    "\n",
    "        # Patch the first row of the extrapolated dataframe to the last row of the actual chain data, to facilitate interpolation\n",
    "        # There will therefore be 301 interpolated points in the epoch-wise extrapolated dataframe\n",
    "        df_new.loc[0, 'raw_bytes_network_power'] = df['raw_bytes_network_power'].iloc[-1]\n",
    "        df_new['raw_bytes_network_power'] = df_new['raw_bytes_network_power'].interpolate(method='linear', limit_direction='forward')\n",
    "\n",
    "        dynamic_baseline = lambda epoch: df.new_baseline_power.iloc[0] * np.exp(np.log(2)/525600/2*epoch)\n",
    "        g = np.log(2)/31536000*30 # the growth rate in epoch units\n",
    "        b0 = 2.888888 * 2**60 / 2**50\n",
    "\n",
    "        # Don't forget to drop the duplicate row in df_new before concat\n",
    "        df_out = pd.concat([df[['height', 'datetime', 'raw_bytes_network_power']], \n",
    "                            df_new[['height', 'datetime', 'raw_bytes_network_power']].iloc[1:]]) \n",
    "        df_out = df_out.reset_index(drop=True)\n",
    "        df_out['baseline_power'] = dynamic_baseline(np.arange(len(df_out)))\n",
    "        df_out['cumsum_capped_rb_power'] = df_out[['raw_bytes_network_power', 'baseline_power']].min(axis=1).cumsum()\n",
    "        df_out['est_network_height'] = 15410 + 1.055*np.log(df_out['cumsum_capped_rb_power']*g/b0 + 1)/g  \n",
    "\n",
    "\n",
    "    else: \n",
    "\n",
    "        assert feature in df.columns, \"%s not in epoch-level dataframe\" % feature\n",
    "\n",
    "        # Populate a new epoch-wise dataframe for the forward predictions\n",
    "        df_new = pd.DataFrame()\n",
    "        df_new['height'] = np.arange(df.height.max(), n_days*2880+df.height.max())\n",
    "        df_new['datetime'] = df_new['height'].apply(height_to_timestamp)\n",
    "\n",
    "        # Impute the daily prediction at 12 midnight of each day\n",
    "        midnight_condition = (df_new['datetime'].dt.hour == 0) & (df_new['datetime'].dt.minute == 0) & (df_new['datetime'].dt.second == 0)\n",
    "        df_new[feature] = np.nan\n",
    "\n",
    "        midnight_idx = np.argwhere(midnight_condition.values>0).reshape(1,-1)[0]\n",
    "        for i,idx in enumerate(midnight_idx):\n",
    "            df_new.loc[idx, feature] = df_p.iloc[-n_days:]['yhat'].values[i]\n",
    "\n",
    "        # Patch the first row of the extrapolated dataframe to the last row of the actual chain data, to facilitate interpolation\n",
    "        # There will therefore be 301 interpolated points in the epoch-wise extrapolated dataframe\n",
    "        df_new.loc[0, feature] = df[feature].iloc[-1]\n",
    "        df_new[feature] = df_new[feature].interpolate(method='linear', limit_direction='forward')\n",
    "\n",
    "        # Don't forget to drop the duplicate row in df_new before concat\n",
    "        df_out = pd.concat([df[['height', 'datetime', feature]], \n",
    "                            df_new[['height', 'datetime', feature]].iloc[1:]]) \n",
    "        df_out = df_out.reset_index(drop=True)\n",
    "\n",
    "        if feature == 'qa_bytes_network_power':\n",
    "            # if the feature is QA bytes network power, re-calculate the smoothed velocity and position estimates for the entire extrapolated df\n",
    "\n",
    "            position = df['qa_smoothed_position_estimate'].iloc[0]\n",
    "            velocity = df['qa_smoothed_velocity_estimate'].iloc[1]\n",
    "            dt = 1\n",
    "            alpha = 0.000164\n",
    "            beta = 0.000115\n",
    "            qa_recalc_pos = []\n",
    "            qa_recalc_vel = []\n",
    "            for k in range(len(df_out)):\n",
    "                position = position + velocity * dt\n",
    "                velocity = velocity\n",
    "                residual = df_out['qa_bytes_network_power'].iloc[k] - position\n",
    "                position = position + alpha * residual\n",
    "                velocity = velocity + (beta/dt) * residual\n",
    "                qa_recalc_pos.append(position)\n",
    "                qa_recalc_vel.append(velocity)\n",
    "            df_out['qa_smoothed_position_estimate'] = qa_recalc_pos\n",
    "            df_out['qa_smoothed_velocity_estimate'] = qa_recalc_vel\n",
    "\n",
    "            df_out = df_out[['height', 'datetime', 'qa_bytes_network_power', 'qa_smoothed_position_estimate', 'qa_smoothed_velocity_estimate']]\n",
    "\n",
    "    return df_out\n",
    "\n",
    "def calculate_block_reward(df):\n",
    "\n",
    "    '''\n",
    "    Given a fully extrapolated epoch-level dataframe, calculate block rewards received per day. \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df (pd.DataFrame): extrapolated epoch-level dataframe\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df (pd.DataFrame): dataframe with simple reward, baseline reward, M (minted supply target), w (reward per wincount)\n",
    "    '''\n",
    "\n",
    "    FIL_BASE = 2e9          # only this amount in FIL will ever be issued\n",
    "    M_infty = 0.55*FIL_BASE # asymptotic number of tokens\n",
    "    lda = np.log(2)/6       # the minting rate corresponding to a 6 year half life\n",
    "    gamma = 0.7             # the minting ratio assigned to hybrid minting\n",
    "    conv_factor = 525600*2  # number of epochs in a year\n",
    "\n",
    "    df['delta_t'] = ( df['height'] - df['height'].iloc[0])\n",
    "    df['delta_t_bl'] = ( df['est_network_height'] - df['est_network_height'].iloc[0])    \n",
    "    df['simple_reward'] = (1-gamma)*M_infty*gamma*(1-np.exp(-lda * df['delta_t']/conv_factor))\n",
    "    df['baseline_reward'] = gamma*M_infty*gamma*(1-np.exp(-lda * df['delta_t_bl']/conv_factor))\n",
    "    df['M'] =  df['simple_reward'] + df['baseline_reward']\n",
    "    \n",
    "    # The rolling window is arbitrary\n",
    "    df['M_diff'] = df['M'].diff().rolling(2880).mean() #np.gradient(df['M'].rolling(2880).mean())\n",
    "    df['w'] = df['M_diff'] / 5 \n",
    "\n",
    "    return df[['height', 'datetime', 'M', 'w']]\n",
    "\n",
    "def ratio_extract(num_p, num_v, denom_p, denom_v, delta_T):\n",
    "    '''\n",
    "    Evaluate the ratio of two variables by the alpha-beta filter\n",
    "    '''\n",
    "    x2a = np.log(denom_p + denom_v) \n",
    "    x2b = np.log(denom_p + denom_v + denom_v * delta_T)\n",
    "    m1 = denom_v * num_p * (x2b - x2a)\n",
    "    m2 = num_v * (denom_p * (x2a - x2b) + denom_v * delta_T)\n",
    "    return (m1 + m2) / denom_v**2    \n",
    "\n",
    "def predict_on_dataframe(df, column_name = 'raw_bytes_network_power', n_days=450):\n",
    "    dc = df[['datetime', column_name]].reset_index(drop=True)\n",
    "    dc.columns = ['ds', 'y']\n",
    "    \n",
    "    m = Prophet(daily_seasonality=False)\n",
    "    with suppress_stdout_stderr():\n",
    "        m.fit(dc)\n",
    "    future = m.make_future_dataframe(periods=450)\n",
    "    fcst = m.predict(future)\n",
    "    return fcst\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ea810571-ef73-4b07-b7c8-aa7a11ad136d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Load data\n",
    "\n",
    "Epoch-wise data is generated from the following query\n",
    "\n",
    "```sql\n",
    "with WinTime as (\n",
    "    select \n",
    "      height,\n",
    "      sum(win_count) as win_count\n",
    "    from lily.block_headers\n",
    "    where height > 148888\n",
    "    group by height\n",
    "),\n",
    "Chain as (\n",
    "select\n",
    "    cr.height,\n",
    "    first(cr.effective_network_time) AS network_height,\n",
    "\tavg(cr.new_baseline_power / power(2,50)) as new_baseline_power,    \n",
    "\tavg(cp.total_raw_bytes_power / power(2,50)) as raw_bytes_network_power, \n",
    "    avg(cp.total_qa_bytes_power / power(2,50)) as qa_bytes_network_power,\n",
    "    avg(cp.qa_smoothed_position_estimate/power(2,50)/power(2,128)) as qa_smoothed_position_estimate,\n",
    "    avg(cp.qa_smoothed_velocity_estimate/power(2,50)/power(2,128)) as qa_smoothed_velocity_estimate,\n",
    "    avg(cr.new_reward_smoothed_position_estimate/1e18/power(2,128)) as new_reward_smoothed_position_estimate,\n",
    "    avg(cr.new_reward_smoothed_velocity_estimate/1e18/power(2,128)) as new_reward_smoothed_velocity_estimate,\n",
    "    avg(cr.new_reward/1e18) as new_reward,\n",
    "    avg(ce.circulating_fil/1e18) as circulating_fil\n",
    "    \n",
    "FROM lily.chain_rewards cr\n",
    "join lily.chain_powers cp on cp.height = cr.height\n",
    "join lily.chain_economics ce on cr.height = ce.height\n",
    "where cr.height > 148888\n",
    "group by cr.height\n",
    ")\n",
    "select \n",
    "    Chain.height,\n",
    "    Chain.network_height,\n",
    "    Chain.new_baseline_power,\n",
    "    Chain.raw_bytes_network_power,\n",
    "    Chain.qa_bytes_network_power,\n",
    "    Chain.qa_smoothed_position_estimate,\n",
    "    Chain.qa_smoothed_velocity_estimate,\n",
    "    Chain.new_reward_smoothed_position_estimate,\n",
    "    Chain.new_reward_smoothed_velocity_estimate,\n",
    "    Chain.new_reward,\n",
    "    Chain.circulating_fil,\n",
    "    WinTime.win_count\n",
    "from Chain\n",
    "join WinTime\n",
    "on Chain.height = WinTime.height\n",
    "order by Chain.height asc \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b5892032-5b50-4d69-827b-a0f4cf85d021",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Making forward predictions\n",
    "\n",
    "I use Facebook Prophet to anticipate what the raw byte and quality adjusted network powers will be in the future. For convenience, I also make a projection of circulating supply, but there's no actual need for that as the circulating supply increases at a predictable and linear fashion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9bcdc37f-bef6-4b92-ab68-84702210c800",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 0. Load data\n",
    "\n",
    "df = pd.read_csv('data/df_height.csv', index_col=[0])\n",
    "df['datetime'] = df['height'].apply(height_to_timestamp)\n",
    "\n",
    "# A new dataframe that's sampled exactly at midnight\n",
    "# it should occur at intervals of 2880 epochs or so \n",
    "dfc = df[(df['datetime'].dt.hour == 0) & (df['datetime'].dt.minute == 0) & (df['datetime'].dt.second == 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b87378ba-7dd7-4328-9e53-b152fff381ac",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. Run separate predictions on RB and QA network power (daily frequency)\n",
    "fcst = predict_on_dataframe(dfc, 'raw_bytes_network_power')\n",
    "fcst_qa = predict_on_dataframe(dfc, 'qa_bytes_network_power')\n",
    "fcst_circ = predict_on_dataframe(dfc, 'circulating_fil')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,3))\n",
    "fcst[['yhat_upper', 'yhat', 'yhat_lower']].plot(ax=ax)\n",
    "ax.set_title('Forward prediction of real byte network capacity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "07ea01ed-8c4b-44a3-9d53-ee37b15b46a7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 2. Interpolate the predictions from daily to epoch level \n",
    "df_new = interpolate_predictions(df, fcst, 'raw_bytes_network_power')\n",
    "df_new_qa = interpolate_predictions(df, fcst_qa, 'qa_bytes_network_power')\n",
    "df_new_circ = interpolate_predictions(df, fcst_circ, 'circulating_fil')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1d0ceb1c-5b3e-4684-bbe2-11199f536498",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Now let's check that the calculations are working as expected and that the interpolations make sense:\n",
    "* There should be a second baseline crossing around Feb-Apr 2023\n",
    "* The calculated and original network heights should overlap as perfectly as possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "49c271da-120d-47cc-ad78-8a186c1b4125",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "latest_chain_date = df.datetime.iloc[-1]\n",
    "\n",
    "# 2a. Inspect the interpolations made \n",
    "fig, (ax1,ax2,ax3) = plt.subplots(1,3, figsize=(15,3))\n",
    "ax1.plot(df_new['datetime'], df_new['baseline_power'], label='Baseline')\n",
    "ax1.plot(df_new['datetime'], df_new['raw_bytes_network_power'], label='RB power')\n",
    "ax1.plot([latest_chain_date, latest_chain_date], [0, df_new.baseline_power.max()], 'k--', lw=0.5)\n",
    "ax1.legend(loc='upper left')\n",
    "ax2.plot(df_new['datetime'], df_new['est_network_height'], label=\"Extrapolated network height\")\n",
    "ax2.plot(df['datetime'], df['network_height'], label='Original network height')\n",
    "ax2.legend(loc='upper left')\n",
    "ax3.plot(df_new['datetime'], df_new['raw_bytes_network_power'], label='RB power')\n",
    "ax3.plot(df_new_qa['datetime'], df_new_qa['qa_bytes_network_power'], label='QA power')\n",
    "ax3.plot([latest_chain_date, latest_chain_date], [0, df_new_qa.qa_bytes_network_power.max()], 'k--', lw=0.5)\n",
    "ax3.legend(loc='upper left')\n",
    "ax1.set_ylabel('Power (PiB)')\n",
    "ax2.set_ylabel('Height (epoch)')\n",
    "ax3.set_ylabel('Power (PiB)')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f9885408-993c-444f-ba37-48babe59984d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_new = interpolate_predictions(df, fcst, 'raw_bytes_network_power')\n",
    "df_out = calculate_block_reward(df_new)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,3))\n",
    "ax.plot(df_out['datetime'].iloc[0:len(df)], 1.4*df_out['w'].iloc[0:len(df)], label='Current')\n",
    "ax.plot(df_out['datetime'].iloc[len(df):], 1.4*df_out['w'].iloc[len(df):], label='Predicted', color='C1')\n",
    "ax.set_ylabel('Reward per WinCount')\n",
    "ax.set_xlabel('Date')\n",
    "ax.legend(loc='upper right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "202fb027-267e-44c1-b60b-3caecb79aec4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,3))\n",
    "\n",
    "for yh,lbl in zip(['yhat_lower', 'yhat', 'yhat_upper'], ['pessimistic', 'median', 'optimistic']):\n",
    "\n",
    "    df_new = interpolate_predictions(df, fcst, 'raw_bytes_network_power', yh)\n",
    "    df_out = calculate_block_reward(df_new)\n",
    "    ax.plot(df_out['datetime'].iloc[len(df):], 1.4*df_out['w'].iloc[len(df):], label='Predicted (%s)'%lbl)\n",
    "\n",
    "ax.plot(df_out['datetime'].iloc[0:len(df)], 1.4*df_out['w'].iloc[0:len(df)], label='Current')\n",
    "ax.set_ylabel('Reward per WinCount $w(t)$')\n",
    "ax.set_xlabel('Date')\n",
    "ax.legend(loc='upper right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f234dea3-0edf-4b11-bffb-2387edfc9ac5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Calculate initial pledge\n",
    "\n",
    "To perform a forward prediction of initial pledge, we need to: \n",
    "\n",
    "* recalculate the `qa_smoothed_position` estimates in our extrapolated data, at epoch level\n",
    "* perform a forward prediction of the circulating supply at epoch level\n",
    "    * not very controversial - circulating supply increases nearly linearly \n",
    "* perform a forward prediction of QA power and baseline power at epoch level\n",
    "    * possibly a more principled way is to have some model of sector expiry\n",
    "    * that way we only predict real byte network power and extrapolate to QA power as well\n",
    "    * but my main priority is to use as few adjustable parameters as possible\n",
    "* perform a forward prediction of WinCount at epoch level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fd1dab01-b3a9-4f51-bbfc-3553de780019",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# (optional) Recalculate QA smoothing - this is already done in our function\n",
    "position = df['qa_smoothed_position_estimate'].iloc[0]\n",
    "velocity = df['qa_smoothed_velocity_estimate'].iloc[1]\n",
    "dt = 1\n",
    "alpha = 0.000164\n",
    "beta = 0.000115\n",
    "qa_recalc_pos = []\n",
    "qa_recalc_vel = []\n",
    "for k in range(len(df)):\n",
    "    position = position + velocity * dt\n",
    "    velocity = velocity\n",
    "    residual = df['qa_bytes_network_power'].iloc[k] - position\n",
    "    position = position + alpha * residual\n",
    "    velocity = velocity + (beta/dt) * residual\n",
    "    qa_recalc_pos.append(position)\n",
    "    qa_recalc_vel.append(velocity)\n",
    "\n",
    "fig, (ax1,ax2) = plt.subplots(1,2,figsize=(12,3))\n",
    "ax1.plot(pd.Series(qa_recalc_pos).rolling(2880*2).mean(), label='Recalculated')\n",
    "ax1.plot(df['qa_smoothed_position_estimate'].values, label='Chain')\n",
    "\n",
    "ax2.plot(pd.Series(qa_recalc_vel).rolling(2880*2).mean(), label='Recalculated')\n",
    "ax2.plot(df['qa_smoothed_velocity_estimate'].values, label='Chain')\n",
    "ax2.set_yscale('log')\n",
    "ax1.legend(loc='upper left')\n",
    "ax2.legend(loc='lower left')    \n",
    "plt.suptitle('QA power $\\\\alpha\\\\beta$ filter recalculation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d9986ea6-a470-43f5-97fc-b59643ccc3be",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# (optional) Recalculate new_reward smoothing - this is already done in our function\n",
    "position = df['new_reward_smoothed_position_estimate'].iloc[0]\n",
    "velocity = df['new_reward_smoothed_velocity_estimate'].iloc[1]\n",
    "dt = 1\n",
    "alpha = 0.000164\n",
    "beta = 0.000115\n",
    "nr_recalc_pos = []\n",
    "nr_recalc_vel = []\n",
    "for k in range(len(df)):\n",
    "    position = position + velocity * dt\n",
    "    velocity = velocity\n",
    "    residual = df['new_reward'].iloc[k] - position\n",
    "    position = position + alpha * residual\n",
    "    velocity = velocity + (beta/dt) * residual\n",
    "    nr_recalc_pos.append(position)\n",
    "    nr_recalc_vel.append(velocity)\n",
    "\n",
    "fig, (ax1,ax2) = plt.subplots(1,2,figsize=(12,3))\n",
    "ax1.plot(pd.Series(nr_recalc_pos).rolling(2880*2).mean(), label='Recalculated')\n",
    "ax1.plot(df['new_reward_smoothed_position_estimate'].values, label='Chain')\n",
    "\n",
    "ax2.plot(pd.Series(nr_recalc_vel).rolling(2880*2).mean(), label='Recalculated')\n",
    "ax2.plot(df['new_reward_smoothed_velocity_estimate'].values, label='Chain')\n",
    "ax2.set_yscale('log')\n",
    "ax1.legend(loc='upper left')\n",
    "ax2.legend(loc='lower left')    \n",
    "\n",
    "plt.suptitle('New reward $\\\\alpha\\\\beta$ filter recalculation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "871e8ab1-97f4-42fd-b9d2-0014d916edbf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sector_size_pib = 32*1e9/2**50\n",
    "df_out = calculate_block_reward(df_new)\n",
    "df_out['w'].iloc[0] = df['new_reward'].iloc[0]/7.5\n",
    "df_out['w'] = df_out['w'].interpolate(method='linear', limit_direction='forward')\n",
    "\n",
    "# Update the smoothed position and velocity estimates\n",
    "position = df['qa_smoothed_position_estimate'].iloc[0]\n",
    "velocity = df['qa_smoothed_velocity_estimate'].iloc[0]\n",
    "dt = 1\n",
    "alpha = 0.000164\n",
    "beta = 0.000115\n",
    "qa_recalc_pos = []\n",
    "qa_recalc_vel = []\n",
    "for k in range(len(df_new_qa)):\n",
    "    position = position + velocity * dt\n",
    "    velocity = velocity\n",
    "    residual = df_new_qa['qa_bytes_network_power'].iloc[k] - position\n",
    "    position = position + alpha * residual\n",
    "    velocity = velocity + (beta/dt) * residual\n",
    "    qa_recalc_pos.append(position)\n",
    "    qa_recalc_vel.append(velocity)\n",
    "\n",
    "position = df['new_reward_smoothed_position_estimate'].iloc[0]/7.5\n",
    "velocity = df['new_reward_smoothed_velocity_estimate'].iloc[0]/7.5\n",
    "dt = 1\n",
    "alpha = 0.000164\n",
    "beta = 0.000115\n",
    "nr_recalc_pos = []\n",
    "nr_recalc_vel = []\n",
    "for k in range(len(df_out)):\n",
    "    position = position + velocity * dt\n",
    "    velocity = velocity\n",
    "    # note: Lily's new_reward is about 7.5x more than df_out['w']\n",
    "    residual = df_out['w'].iloc[k] - position\n",
    "    position = position + alpha * residual\n",
    "    velocity = velocity + (beta/dt) * residual\n",
    "    nr_recalc_pos.append(position)\n",
    "    nr_recalc_vel.append(velocity)\n",
    "\n",
    "# Storage pledge is the 20 day block reward per epoch per unit of QAP    \n",
    "storage_pledge = sector_size_pib * ratio_extract(\n",
    "    np.array(nr_recalc_pos), \n",
    "    np.array(nr_recalc_vel),\n",
    "    np.array(qa_recalc_pos),\n",
    "    np.array(qa_recalc_vel),\n",
    "    20 * 1440 * 2)\n",
    "\n",
    "# Delete intermediate variables to save memory\n",
    "del qa_recalc_pos, qa_recalc_vel, nr_recalc_pos, nr_recalc_vel\n",
    "\n",
    "df_ec = pd.DataFrame()\n",
    "df_ec['consensus_pledge'] = (0.3*df_new_circ['circulating_fil'] * sector_size_pib \\\n",
    "                             / pd.DataFrame(np.array([df_new['baseline_power'], df_new_qa['qa_bytes_network_power']]).T).max(axis=1))\n",
    "df_ec['storage_pledge'] = storage_pledge\n",
    "df_ec['storage_pledge'] = df_ec['storage_pledge'].rolling(2880).mean()\n",
    "df_ec['storage_pledge'].iloc[0] = 0.02\n",
    "df_ec['storage_pledge'] = df_ec['storage_pledge'].interpolate(method='linear', limit_direction='forward')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cb4d3df3-7306-406c-b88d-57528c452a7c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,3))\n",
    "ax.plot(df_new['datetime'], df_ec['consensus_pledge'], label='Consensus pledge')\n",
    "ax.plot(df_new['datetime'], df_ec['storage_pledge'], label='Storage pledge')\n",
    "ax.plot(df_new['datetime'], df_ec['storage_pledge'] + df_ec['consensus_pledge'], label='Total initial pledge')\n",
    "ax.set_ylabel('Initial pledge (FIL)')\n",
    "ax.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b9867885-29f9-4db2-934b-28782bebe7f8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### ROI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5efcc16f-21e8-42a5-9a14-75b98907d366",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fig, (ax1,ax2) = plt.subplots(2,1,figsize=(12,5))\n",
    "extended_wincount = np.concatenate([df['win_count'].values, poisson.rvs(mu=5, size=len(df_out)-len(df))])\n",
    "(df_out['w']*extended_wincount).plot(ax=ax1)\n",
    "(df_ec['storage_pledge'] + df_ec['consensus_pledge']).plot(ax=ax2, label='Initial pledge')\n",
    "ax1.set_ylabel('Block reward (FIL per w.c.)')\n",
    "ax2.set_ylabel('Initial pledge (FIL)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6315fe12-b78d-41b0-be38-1fcacaa88790",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Calculating FIL on FIL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "88f857fd-058a-4108-965a-8dd5a06a69d2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Now that we have both the initial collateral and a prediction of collected rewards, we can now check the anticipated return. \n",
    "\n",
    "Currently this number is an overestimate, as only 25% of the rewards are immediately issued, while 75% is locked up and disbursed over linear vesting schedule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3316d8ce-2a76-4272-b8d5-cc1f0a7ad90a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def plot_fil_on_fil(df, df_new, df_new_qa, df_ec, n_days=180):\n",
    "\n",
    "    '''\n",
    "    Plots the FIL on FIL return, using the dataframes calculated earlier.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df_new (pd.DataFrame): epoch-level prediction of raw byte power\n",
    "    df_new_qa (pd.DataFrame): epoch-level prediction of quality adjusted power\n",
    "    df_ec (pd.DataFrame): calculation of the initial pledge\n",
    "    n_days (int): lookahead period\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    '''\n",
    "\n",
    "    # Assume the disbursed reward is in proportion to the sector's share of the QA power \n",
    "    # (as claimed here https://filecoin.io/blog/posts/filecoin-features-block-rewards/)\n",
    "    extended_wincount = np.concatenate([df['win_count'].values, poisson.rvs(mu=5, size=len(df_out)-len(df))])\n",
    "    collected_reward = (sector_size_pib / df_new_qa['qa_bytes_network_power'] * df_out['w'] * extended_wincount).rolling(n_days*2880).sum() # df_new_qa['qa_bytes_network_power'], df_new['raw_bytes_network_power']\n",
    "    fil_on_fil = 100*collected_reward / (df_ec['storage_pledge'] + df_ec['consensus_pledge'])\n",
    "\n",
    "    fig, (ax1,ax2) = plt.subplots(2,1,figsize=(12,5))\n",
    "\n",
    "    # When plotting we need to slice off the last n_days*2880 epochs as the rolling sum is incomplete\n",
    "    ax1.plot(df_new['datetime'].iloc[n_days*2880:-n_days*2880], (df_ec['storage_pledge'] + df_ec['consensus_pledge']).iloc[n_days*2880:-n_days*2880], label='Initial collateral')\n",
    "    ax1.plot(df_out['datetime'].iloc[0:len(collected_reward)-n_days*2880], collected_reward[0:-n_days*2880], label='Collected reward')\n",
    "    ax2.plot(df_out['datetime'].iloc[0:len(collected_reward)-n_days*2880], fil_on_fil[0:-n_days*2880])\n",
    "    ax1.set_ylabel('Value (FIL)')\n",
    "    ax2.set_ylabel('FIL on FIL (%)')\n",
    "    ax1.plot([df_new['datetime'].iloc[len(df)], df_new['datetime'].iloc[len(df)]], [0, 0.3], 'k--', label='Second crossover', lw=0.5)\n",
    "    ax2.plot([df_new['datetime'].iloc[len(df)], df_new['datetime'].iloc[len(df)]], [0, np.max(fil_on_fil)], 'k--', label='Second crossover', lw=0.5)\n",
    "    # ax2.set_ylim([0,25])\n",
    "    ax1.set_ylim([0,0.3])\n",
    "    ax1.legend(loc='upper left')\n",
    "    plt.suptitle('%s day FIL-on-FIL return for a 32 GiB sector' % n_days)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9bc9b55b-7b43-4495-9468-5bbe6ce0438a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "plot_fil_on_fil(df, df_new, df_new_qa, df_ec, 180)\n",
    "plt.xlim([pd.to_datetime('2022-06-01'), pd.to_datetime('2023-04-01')])\n",
    "plt.ylim([0,50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "38e33265-d569-4975-ab90-56dfe81f541a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "plot_fil_on_fil(df, df_new, df_new_qa, df_ec, 360)\n",
    "plt.xlim([pd.to_datetime('2022-06-01'), pd.to_datetime('2023-04-01')])\n",
    "plt.ylim([0,150])"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "FIL on FIL calculator",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
